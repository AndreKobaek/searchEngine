\section{Finding Similar Websites}
\paragraph{K-means}
The K-means algorithm is an iterative method for vector quantization that creates K-number of clusters and calculates the distance between the chosen clusters and the vectors. 
The algorithm utilizes centroids which can be considered the most representative points in a cluster, these are the arithmetic mean of all the points in the vectors.
\paragraph{Algorithm walk-through}
Initially a K-number of centroids are randomly chosen among a given set of observations $(x_0,x_1,x_2,...,x_n)$ which represent the vectors contained in the dataset. These centroids are the reference to the initial clusters. Once the initial centroids are chosen each vector in the dataset is assigned to its closest centroid. This operation creates the initial clusters. Then the centroids are recalculated using the mean of the data point vectors inside the cluster. 
The algorithm iteratively reassigns vectors to more optimal centroids as these are created.\cite{wikiKmeans}
This process continues until the last $n$ and $n-1$ iterations have centroids with the same values. 
This constraint ensures that a steady state has been reached and further iterations are pointless.
Because of this, it is crucial how the vector is built. To extract a vector out of text (in this case the text from the website) we need to be able to assign a weight to each word from a website. This is used to perform computational operations on it. We used the vector space model which represents a website as a set of words and used the TF-IDF to assign weights to each word. As explained in section \ref{sec:rankingAlgorithm} the TF-IDF gives us a measure of how  relevant a word to a certain website. The values of these calculations are stored in the vectors.
Once we have built the vectors we need to measure the distance between the vector and a centroid, this can be achieved using different methods. In our case, we calculated the cosine similarity which measures the distance of two vectors calculating the cosine of the angle between them. The result is a number between $-1.0$ and $1.0$. Where $-1.0$ means that the vectors are diametrically opposed, hence there is no similarity between the website and the centroid and $1.0$ means the vectors have the same orientation, hence the website and the centroid have the same values.\cite{wikiVector}
$$
\cos(\theta) = \frac{A \cdot B}{\Vert A \Vert \Vert B \Vert} = \frac{\sum_{i=1}^n A_iB_i}{\sqrt{\sum_{i=1}^n A_i^2}{\sqrt{\sum_{i=1}^n B_i^2}}}
$$
\paragraph{Algorithm implementation}The clustering has two different implementation. In the first one (including \code{Kmeans}, \code{Centroid}, \code{Vector} and \code{CosineSimilarity} classes) the vectors ordering and the relative centroids rely on the order of the indices into lists. This kind of implementation results in lists where if a word is not present it is assigned weight $0$ from the TF-IDF. This creates very large arrays with many $0$ entries, resulting in a computationally expensive building process. There is indeed a problem when building the application with the medium and large database because is needed more memory allocation on the heap. To overcome this problem we created another implementation of the same algorithm that works in the same way, the only difference is that it uses maps instead of lists. In this way we can avoid to rely on indices order for retrieving elements with $0$ as a weigth. In this case we store a value in a Map, with the related word, only if it is different from $0$. This implementation is represented by the \code{KmeansMap}, the \code{CentroidMap}, the \code{VectorMap} and the \code{CosineSimilarityMap} classes.
In order to allocate more memory we changed the gradle-wrappers.property file, configuring the JVM memory as showed into the gradle documentation\cite{gradleDocs} to place 3 gigabyte of ram when building the project.


The \code{KMeans} and \code{KMeansMap} classes are in charge of running the entire algorithm. When the \code{startKmeans} method is called it takes an \code{int k} corresponding to the number of clusters that will be created. The algorithm calculates the initial centroids, it calculates the cosine similarity using \code{CosineSimilarity} or \code{CosineSimilarityMap} class and inserts the results into vector objects which contain the relative website with all the values and the \code{Centroid} or \code{CentroidMap} class which contain the centroid values and represents the cluster. After the initial set up, the algorithm enters into a loop which recalculates the centroids until they achieve an optimal steady state, as previously explained. To avoid an infinite loop a method is called to check the difference between the last two calculated centroids at the end of every loop iteration.

\paragraph{Conclusion}Implementing this algorithm has been an occasion to approach an important topic for cluster analysis in data mining. We learnt the basic principles for classifying texts documents applying interdisciplinary knowledge between programming in Java and mathematics.
We understood our errors in the design of the first implementation and tried to overcome its limits with a new design.

The implementation works as expected and we noticed that for the small database, working with both the implementations, a $k$ of 150 is the minimum optimal value to create reliable clusters that show a relation between the different sets of websites. Regarding the medium size database our observations have been limited from the time needed to build the applications with such a big dataset. In this case only the implementation with the \code{Map} works and it took more than one hour to build the first time with a $k$ set to 15 and more than four hours with a $k$ set to 200. 
