\section{Finding Similar Websites}
\paragraph{K-means}
The K-means algorithm is an iterative method for vector quantization that creates K-number of clusters and calculates the distance between the chosen clusters and the vectors. 
The algorithm utilizes centroids which are ... \\
\paragraph{Algorithm walk-through}
Initially a K-number of centroids are randomly chosen among a given set of observations $(x_0,x_1,x_2,...,x_n)$ which represent the vectors contained in the dataset. These centroids are the reference to the initial clusters. Once the initial centroids are chosen each vector in the dataset is assigned to its closest centroid. This operation creates the initial clusters. Then the centroids are recalculated using the mean of the data point vectors inside the cluster. 
The algorithm iteratively reassigns vectors to more optimal centroids as these are created.
This process continues until the last $n$ and $n-1$ iterations have centroids with the same values. 
This constraint ensures that a steady state has been reached and further iterations are pointless.
Because of this it is crucial how the vector is built. To extract a vector out of text (in this case the text from the website) we need to be able to assign a weight to each word from a website. This is used to perform computational operations on it. We used the vector space model which represents a website as a set of words and used the TF-IDF to assign weights to each word. As explained in section \ref{//section??} the TF-IDF gives us a measure of how  relevant a word to a certain website. The values of these calculations are stored in the vectors.
Once we have built the vectors we need to measure the distance between the vector and a centroid, this can be achieved using different methods. In our case, we calculated the cosine similarity which measures the distance of two vectors calculating the cosine of the angle between them. The result is a number between $-1.0$ and $1.0$. Where $-1.0$ means that the vectors are diametrically opposed, hence there is no similarity between the website and the centroid and $1.0$ means the vectors have the same orientation, hence the website and the centroid have the same values.
$$
\cos(\theta) = \frac{A \cdot B}{\Vert A \Vert \Vert B \Vert} = \frac{\sum_{i=1}^n A_iB_i}{\sqrt{\sum_{i=1}^n A_i^2}{\sqrt{\sum_{i=1}^n B_i^2}}}
$$
The clustering is implemented using a \code{K-means} class which is in charge of running the entire algorithm. When the K-means method is called it takes an \code{int k} corresponding to the number of clusters that will be created. The algorithm calculates the initial centroids, calculate the cosine similarity using \code{CosineSimilarity} class and insert the results into vector objects which contain the relative website with all the values and the \code{Centroid} class which contain the centroid values and represents the cluster. After the initial set up, the algorithm enters into a loop which recalculates the centroids until they achieve an optimal steady state, as previously explained. To avoid an infinite loop a method is called to check the difference between the last two calculated centroids at the end of every loop iteration.

Implementing this algorithm has been an occasion to approach an important topic for cluster analysis in data mining. We learnt the basic principles for classifying texts documents applying interdisciplinary knowledge between programming in Java and mathematics.
The implementation works as expected and we noticed that the clusters created are reliable and shows a relation between the different sets of websites.
Despite our efforts, there remain some limits to overcome in future implementations of the program. In particular, the vectors ordering and the relative centroids rely on the order of the indices into lists. This kind of implementation results in lists where if a word is not present it is assigned weight $0$ from the TF-IDF. This creates very large arrays with many $0$ entries, resulting in a computationally expensive building process. There is indeed a problem when building with the medium and large database because is needed more memory allocation on the heap.
Our approach would be to change the index based implementation with a map based one, in this way every time a word has weight $0$, it will not be assigned to the array. We think this would be a better solution in terms of performance and the overall application.